{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49965005-df9d-4e26-9203-67bd2538591c",
   "metadata": {},
   "source": [
    "# Stateless / statefull  Word count Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1730920-5d3c-490f-bc05-211565507bf6",
   "metadata": {},
   "source": [
    "## Import librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a1257-8c8e-4671-b0e5-8c0376099d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init('C:\\spark\\spark-2.3.0-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff3f243-7ddc-498d-856d-b3f47ebd7cc8",
   "metadata": {},
   "source": [
    "## Set Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c85bf7-0024-4ab8-a803-772890f3ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALA_VERSION = '2.11'\n",
    "SPARK_VERSION = '2.3.0'\n",
    "spark_path = r\"C:\\spark\\spark-2.3.0-bin-hadoop2.7\" # spark installed folder\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS']=\"--master local[2] pyspark-shell\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']='--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS']='C:\\spark\\spark-2.3.0-bin-hadoop2.7\\jars\\spark-streaming-kafka-0-8-assembly_2.11-2.3.0-preview.jar'\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell'\n",
    "sys.path.insert(0, spark_path + \"/bin\")\n",
    "sys.path.insert(0, spark_path + \"/python/pyspark/\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.insert(0, spark_path + \"/python/lib/py4j-0.10.6-src.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b9211-3c11-451f-84e1-6df1b33c949f",
   "metadata": {},
   "source": [
    "## Start Context and Set a checkPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f36baf-d6bb-4f60-89d1-33f61ee5d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "ssc = StreamingContext(sc,60)\n",
    "#ssc.checkpoint(\"file:///C:/tmp/xo\")\n",
    "ssc.checkpoint(\"checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84901c1e-0586-4a69-9fe1-a1401e2976c2",
   "metadata": {},
   "source": [
    "## Read From kafka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0da849-333f-4969-84bf-5a16b489046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = KafkaUtils.createDirectStream(ssc,topics=[\"rdd1\"],kafkaParams={\"metadata.broker.list\":\"localhost:9092\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c3e92-280d-445d-b084-66ecdfec8d2e",
   "metadata": {},
   "source": [
    "## Show stream at screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dec5fe-ec5f-4c7c-917e-08e5d85469a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.pprint()\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e198ac-53c2-4ba7-a240-c2dfce087442",
   "metadata": {},
   "source": [
    "##  count words in each batch without updating state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279289f9-728b-4cdd-972f-5dc0108f58f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch is set 1 s \n",
    "words =lines.map(lambda x : x[1]).flatMap(lambda x: x.split(\" \"))\n",
    "wordcount = words.map(lambda x : (x, 1)).reduceByKey(lambda a,b : a+b)\n",
    "\n",
    "wordcount.pprint()\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25970ef7-844f-48d5-845a-a5ae499ce413",
   "metadata": {},
   "source": [
    "## Count word and update state by Using updateBykey "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed73282-13cf-4b85-91af-2cd3123de307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount)\n",
    "\n",
    "words =lines.map(lambda x : x[1]).flatMap(lambda x: x.split(\" \"))\n",
    "wordcount = words.map(lambda x : (x, 1))\n",
    "runningCounts = wordcount.updateStateByKey(updateFunction)\n",
    "\n",
    "runningCounts.pprint()\n",
    "# Start the computation\n",
    "\n",
    "ssc.start()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4073c88-ffbe-494c-bd96-b529f994311f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
